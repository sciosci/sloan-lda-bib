{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2276d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.coherencemodel import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965e4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e2c1448",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77cca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = pd.read_csv('./data/lda_abstacts_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93a75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts['abstract_processed'] = abstracts['abstract'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "abstracts['abstract_processed'] = abstracts['abstract'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46307ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e1e3ca9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c7b81963da79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabstracts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract_processed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-a654b5b4c335>\u001b[0m in \u001b[0;36msent_to_words\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msent_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32myield\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/hzhuang/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msimple_preprocess\u001b[0;34m(doc, deacc, min_len, max_len)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \"\"\"\n\u001b[1;32m    313\u001b[0m     tokens = [\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmin_len\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     ]\n",
      "\u001b[0;32m/home/hzhuang/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text, lowercase, deacc, encoding, errors, to_lower, lower)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeaccent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimple_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hzhuang/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mdeaccent\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NFD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Mn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NFC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = abstracts.abstract_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8cbe0f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-49da2351d87b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Faster way to get a sentence clubbed as a trigram/bigram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbigram_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphrases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhraser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# trigram_mod = gensim.models.phrases.Phraser(trigram)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bigram' is not defined"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=300)\n",
    "# trigram = gensim.models.Phrases(bigram[data_words], threshold=300)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ca503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stop_words = ['use', 'study', 'result', 'also', 'may', 'find', 'method', 'system',\n",
    "                         'however', 'suggest', 'paper', 'include', 'increase', 'solution', 'change', \n",
    "                         'process', 'different', 'base', 'effect', 'rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7244754",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') + additional_stop_words\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf46c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38cfe795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377d389c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['amino', 'acid', 'use', 'protein', 'formation', 'methylation', 'cancer', 'cell', 'require', 'particularly', 'high', 'methionine', 'supply', 'homeostasis', 'successful', 'approach', 'decrease', 'methionine', 'concentration', 'base', 'systemic', 'delivery', 'methionine', 'lyase', 'study', 'demonstrate', 'efficacy', 'cancer', 'therapy', 'mechanism', 'explain', 'cancer', 'cell', 'suffer', 'absence', 'significantly', 'non', 'malignant', 'cell', 'still', 'unclear', 'analyze', 'human', 'colorectal', 'adenocarcinoma', 'cancer', 'cell', 'line', 'exposure', 'monitor', 'cell', 'viability', 'expression', 'histone', 'post', 'translational', 'modification', 'presence', 'spurious', 'transcription', 'rationale', 'verify', 'reduce', 'methionine', 'supply', 'would', 'affect', 'decondensation', 'change', 'level', 'histone', 'methylation', 'therefore', 'increase', 'genomic', 'instability', 'treatment', 'show', 'time', 'dependent', 'cancer', 'cell', 'µg', 'hs', 'normal', 'cell', 'less', 'affected', 'sub', 'sub', 'level', 'total', 'histone', 'methylation', 'alter', 'loss', 'silence', 'histone', 'observe', 'decrease', 'decorate', 'repetitive', 'dna', 'element', 'prove', 'treatment', 'lead', 'increase', 'expression', 'major', 'satellite', 'unit', 'datum', 'indicate', 'select', 'histone', 'methylation', 'mark', 'play', 'major', 'role', 'methionine', 'starvation', 'cancer', 'cell', 'prove', 'treatment', 'directly', 'impact', 'homeostasis']]\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5647d005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 6), (11, 8), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 5), (33, 2), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 1), (46, 2), (47, 1), (48, 1), (49, 1), (50, 5), (51, 4), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 2), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 2), (78, 1), (79, 1), (80, 2), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 3), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f0e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./topic_modeling/corpus_newstop.pickle', 'wb') as c:\n",
    "    pickle.dump(corpus, c)\n",
    "with open('./topic_modeling/id2word_newstop.pickle', 'wb') as i:\n",
    "    pickle.dump(id2word, i)\n",
    "with open('./topic_modeling/data_lemmatized_newstop.pickle', 'wb') as d:\n",
    "    pickle.dump(data_lemmatized, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4290886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./topic_modeling/corpus.pickle', 'wb') as c:\n",
    "#     pickle.dump(corpus, c)\n",
    "# with open('./topic_modeling/id2word.pickle', 'wb') as i:\n",
    "#     pickle.dump(id2word, i)\n",
    "# with open('./topic_modeling/data_words.pickle', 'wb') as d:\n",
    "#     pickle.dump(data_words, d)\n",
    "# with open('./topic_modeling/data_words_bigrams.pickle', 'wb') as d:\n",
    "#     pickle.dump(data_words_bigrams, d)\n",
    "# with open('./topic_modeling/data_lemmatized.pickle', 'wb') as d:\n",
    "#     pickle.dump(data_lemmatized, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a183fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pickle.load(open('./topic_modeling/corpus_newstop.pickle', 'rb'))\n",
    "id2word = pickle.load(open('./topic_modeling/id2word_newstop.pickle', 'rb'))\n",
    "data_lemmatized = pickle.load(open('./topic_modeling/data_lemmatized_newstop.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e661b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=42,\n",
    "                                           chunksize=500)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    temp_file = datapath(\"/home/lliang06/sloan/topic_modeling/lda\" + str(k) + \"d_newstop\")\n",
    "    lda_model.save(temp_file)\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d664c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k= 110\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=k, \n",
    "                                       random_state=42,\n",
    "                                       chunksize=1000,\n",
    "                                       passes = 5,\n",
    "                                       alpha=0.01,\n",
    "                                       eta=0.31\n",
    "                                      )\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "\n",
    "temp_file = datapath(\"/home/lliang06/sloan/topic_modeling/lda\" + str(k) + \"d_tuned_alpha_beta_newstop\")\n",
    "lda_model.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc5ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "623cd918",
   "metadata": {},
   "source": [
    "### Getting LDAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5a131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = pd.read_parquet('potential_abstracts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e1f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_energy_pi_pd = pd.read_csv('./dimension_energy_pi_abstracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab98309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5866978"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "115e2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_abstracts_df = pd.concat([abstracts, dimension_energy_pi_pd]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "957ecf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_abstracts_df['abstract_processed'] = dimension_abstracts_df['abstract'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "dimension_abstracts_df['abstract_processed'] = dimension_abstracts_df['abstract'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_abstracts_df[['publication_id', 'abstract_processed']].to_parquet('./all_abstracts.parquet', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d3135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_abstracts_df = pd.read_parquet('./all_abstracts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce0c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "additional_stop_words = ['use', 'study', 'result', 'also', 'may', 'find', 'method', 'system',\n",
    "                         'however', 'suggest', 'paper', 'include', 'increase', 'solution', 'change', \n",
    "                         'process', 'different', 'base', 'effect', 'rate']\n",
    "        \n",
    "stop_words = stopwords.words('english') + additional_stop_words\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30d0052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb6a4b6a7d64e238b5afa220643b2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(dimension_abstracts_df) // 1000000 + 1)):\n",
    "    left = i * 1000000\n",
    "    right = i * 1000000 + 999999\n",
    "    \n",
    "    data = dimension_abstracts_df.loc[left:right].abstract_processed.values.tolist()\n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=300)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    \n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    with open('./full_abstracts/data_full_abstracts_' + str(i) + '.pickle', 'wb') as c:\n",
    "        pickle.dump(data_lemmatized, c)\n",
    "\n",
    "#     id2word = corpora.Dictionary(data_lemmatized)\n",
    "    \n",
    "#     corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n",
    "    \n",
    "#     with open('./full_abstracts/corpus_full_abstracts_' + str(i) + '.pickle', 'wb') as c:\n",
    "#         pickle.dump(corpus, c)\n",
    "\n",
    "        \n",
    "    del data\n",
    "    del data_words\n",
    "    del bigram\n",
    "    del bigram_mod\n",
    "    del data_words_nostops\n",
    "    del data_words_bigrams\n",
    "    del data_lemmatized\n",
    "#     del id2word\n",
    "#     del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e09964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = pickle.load(open('./topic_modeling/corpus_sampled_PI_old_id.pickle', 'rb'))\n",
    "id2word = pickle.load(open('./topic_modeling/id2word_newstop.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623f2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = datapath(\"/home/lliang06/sloan/topic_modeling/lda110d_tuned_alpha_beta_newstop\")\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74f66e27",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d3e7d8a87c49c9abde04610f55dd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2210417f864f15ad5fb2775759477e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6caa617c9b5045978ce99569679a789e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da05156a3c9e487891e10a828ae6df5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb949bb07f1d4c85bd02cdfa8a3e9cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(7):\n",
    "    data = pickle.load(open('./full_abstracts/data_full_abstracts_' + str(j) + '.pickle', 'rb'))\n",
    "    corpus = [id2word.doc2bow(text) for text in data]\n",
    "    vec_iter = lda.get_document_topics(corpus, minimum_probability = 0)\n",
    "    \n",
    "    for i, v in tqdm(enumerate(vec_iter)):\n",
    "        if ((i % 10000 == 0) | (i == 0)):\n",
    "            if i == 10000:\n",
    "                with open('./full_abstracts/lda_110d_fin_sampled_PI_abstracts_' + str(j) + '.npy', 'wb') as f:\n",
    "                    np.save(f, lda_vec)\n",
    "\n",
    "            if i > 10000:\n",
    "                with open('./full_abstracts/lda_110d_fin_sampled_PI_abstracts_' + str(j) + '.npy', 'rb') as f:\n",
    "                    all_vec = np.load(f)\n",
    "                all_vec = np.vstack([all_vec, lda_vec])\n",
    "                with open('./full_abstracts/lda_110d_fin_sampled_PI_abstracts_' + str(j) + '.npy', 'wb') as f:\n",
    "                    np.save(f, all_vec)\n",
    "\n",
    "            lda_vec = np.array([w[1] for w in v])\n",
    "        else:\n",
    "            lda_vec = np.vstack([lda_vec, np.array([w[1] for w in v])])\n",
    "\n",
    "    with open('./full_abstracts/lda_110d_fin_sampled_PI_abstracts_' + str(j) + '.npy', 'rb') as f:\n",
    "        all_vec = np.load(f)\n",
    "    all_vec = np.vstack([all_vec, lda_vec])\n",
    "\n",
    "    with open('./full_abstracts/lda_110d_fin_sampled_PI_abstracts_' + str(j) + '.npy', 'wb') as f:\n",
    "        np.save(f, all_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d23be332",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_abstracts_df[['publication_id']].to_csv('./lda_pubid.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66af1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b8473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5840690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac17b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc1659a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
